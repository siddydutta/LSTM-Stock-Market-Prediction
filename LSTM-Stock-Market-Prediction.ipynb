{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "Download and install required packages for this notebook:\n",
    "* Captum: Model Interpretability for PyTorch\n",
    "* Optuna: A Hyperparameter Optimization Framework\n",
    "* Kaleido: Generating Static Images for Optuna Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install captum\n",
    "%pip install optuna\n",
    "%pip install kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Library Imports & Constants\n",
    "\n",
    "This section imports all the necessary libraries and defines constants used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from captum.attr import IntegratedGradients\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import (\n",
    "    plot_contour,\n",
    "    plot_intermediate_values,\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "TRAINING_DATA_FILEPATH = 'train.csv'\n",
    "VAL_SPLIT = 10\n",
    "N_COMPANIES = 442\n",
    "N_TRIALS = 50\n",
    "TRIAL_TIMEOUT = None\n",
    "BEST_PARAMS_FILEPATH = 'best_params.json'\n",
    "BEST_MODEL_FILEPATH = 'best_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This section performs an initial analysis of the dataset by visualisaing trends of the stock percentage changes for a few random companies over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(TRAINING_DATA_FILEPATH, index_col='ID')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE_COMPANIES = 5\n",
    "LAST_N_DAYS = 365\n",
    "random_samples = random.sample(data.index.tolist(), N_SAMPLE_COMPANIES)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for company in random_samples:\n",
    "    plt.plot(data.columns[-LAST_N_DAYS:], data.loc[company, data.columns[-LAST_N_DAYS:]], label=company)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Percentage Movement')\n",
    "plt.title(f'Time Series of Stock Price Changes for {N_SAMPLE_COMPANIES} Random Companies over {LAST_N_DAYS} Days')\n",
    "plt.xticks([])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_series_trends.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the seasonality, shows that most companies go through a series of ups and downs. However, here for example `company_195` witnessed a significant jump of ~24% in a single day. Additionally, outlier events like Covid affected the values of stock market changes drastically. This exploratory data analysis suggested that perhaps if normalisation was performed for every company, instead of the entire dataset, the effect of outliers may be reduced.\n",
    "\n",
    "![](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/time_series_trends.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Custom Dataset\n",
    "\n",
    "This section defines the `StockDataset` class, an implementation of PyTorch's `Dataset` to load custom data as done in Lab 5.\n",
    "It takes in several parameters:\n",
    "* `csv_file_path`: The file path for `train.csv`.\n",
    "* `in_seq_length`: The number of lookback days for the time series prediction.\n",
    "* `is_train`: A flag to indicate if the loader is being used for training or validation.\n",
    "* `train_val_split`: A numeric value indicating how much of the dataset should be used for validation.\n",
    "* `scalers`: In case of validation, the same `StandardScaler` instances from training are re-used to prevent data leakage.\n",
    "\n",
    "As mentioned before, this custom implementation performs a train/validation split and performs normalisation at a per company level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_file_path: str,\n",
    "                 in_seq_length: int,  # number of previous days to consider\n",
    "                 is_train: bool,\n",
    "                 train_val_split: int,  # number of days for validation set\n",
    "                 scalers: Optional[list] = None):\n",
    "        # process raw data\n",
    "        self.df = pd.read_csv(csv_file_path) # (442, 3022)\n",
    "        self.df = self.df.drop(columns=['ID'])  # (442, 3021)\n",
    "        # define parameters\n",
    "        self.in_seq_length = in_seq_length\n",
    "\n",
    "        # train / val split\n",
    "        self.split_idx = self.df.shape[1] - train_val_split\n",
    "        if is_train:\n",
    "            self.data = self.df.iloc[:, :self.split_idx]\n",
    "        else:\n",
    "            self.data = self.df.iloc[:, (self.split_idx - in_seq_length):]\n",
    "\n",
    "        # per company normalisation\n",
    "        self.normalized_data = np.zeros_like(self.data.values)\n",
    "        if scalers is None:\n",
    "            self.scalers = []\n",
    "            for idx in range(self.df.shape[0]):\n",
    "                scaler = StandardScaler()\n",
    "                company_data = self.data.iloc[idx].values.reshape(-1, 1)\n",
    "                self.normalized_data[idx] = scaler.fit_transform(company_data).flatten()\n",
    "                self.scalers.append(scaler)\n",
    "        else:\n",
    "            self.scalers = scalers\n",
    "            for idx in range(self.df.shape[0]):\n",
    "                company_data = self.data.iloc[idx].values.reshape(-1, 1)\n",
    "                self.normalized_data[idx] = self.scalers[idx].transform(company_data).flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of possible sequences\n",
    "        return self.normalized_data.shape[1] - self.in_seq_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # get sequence window\n",
    "        X = self.normalized_data[:, idx:idx + self.in_seq_length]  # (442, seq_length)\n",
    "        # get target values (next day for all companies)\n",
    "        y = self.normalized_data[:, idx + self.in_seq_length]  # (442,)\n",
    "\n",
    "        X = torch.FloatTensor(X.T)  # for LSTM (seq_length, 442)\n",
    "        y = torch.FloatTensor(y)\n",
    "        return {'X': X, 'y': y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LSTM Model\n",
    "\n",
    "This section defines the `StockLSTM` class, a PyTorch implementation of an LSTM model similar to Lab 7.\n",
    "The model initialises the LSTM hidden and cell states to zero, passes through the LSTM and applies a final linear layer to predict the next day's stock movements for all 442 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        # tnitialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_dim)\n",
    "        # take the final time-step\n",
    "        out = out[:, -1, :]  # shape: (batch_size, hidden_dim)\n",
    "        # apply dropout and final linear layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)  # shape: (batch_size, input_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparameter Tuning using Optuna\n",
    "\n",
    "This section uses Optuna to perform hyperparameter optimization for the LSTM model similar to Lab 4. The objective function to optimise here is the mean squared error over the validation dataset. The following hyperparameters are defined:\n",
    "* `sequence_length`: The number of lookback days for the LSTM model.\n",
    "* `hidden_dim`: The dimensions of the latent layers.\n",
    "* `num_layers`: The number of layers in the LSTM.\n",
    "* `dropout`: The dropout rate to prevent overfitting.\n",
    "* `learning_rate`: The learning rate for the optimizer.\n",
    "* `batch_size`: The batch size for training and validation.\n",
    "* `num_epochs`: The number of epochs to train the model.\n",
    "* `optimizer`: The optimisation algorithm to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    PARAMETERS = {\n",
    "        'sequence_length': trial.suggest_int('sequence_length', 3, 150),\n",
    "        'hidden_dim': trial.suggest_int('hidden_dim', 32, 256),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.0, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64, 128]),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 30, 100),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    }\n",
    "    # create datasets\n",
    "    train = StockDataset(\n",
    "        csv_file_path=TRAINING_DATA_FILEPATH,\n",
    "        in_seq_length=PARAMETERS['sequence_length'],\n",
    "        is_train=True,\n",
    "        train_val_split=VAL_SPLIT)\n",
    "    validation = StockDataset(\n",
    "        csv_file_path=TRAINING_DATA_FILEPATH,\n",
    "        in_seq_length=PARAMETERS['sequence_length'],\n",
    "        is_train=False,\n",
    "        train_val_split=VAL_SPLIT,\n",
    "        scalers=train.scalers)\n",
    "    # create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=PARAMETERS['batch_size'],\n",
    "        shuffle=True)\n",
    "    val_loader = DataLoader(\n",
    "        validation,\n",
    "        batch_size=PARAMETERS['batch_size'],\n",
    "        shuffle=False)\n",
    "\n",
    "    # create model\n",
    "    model = StockLSTM(\n",
    "        input_dim=N_COMPANIES,\n",
    "        hidden_dim=PARAMETERS['hidden_dim'],\n",
    "        num_layers=PARAMETERS['num_layers'],\n",
    "        dropout=PARAMETERS['dropout']\n",
    "    ).to(DEVICE)\n",
    "    # loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = getattr(optim, PARAMETERS['optimizer'])(\n",
    "        model.parameters(),\n",
    "        lr=PARAMETERS['learning_rate']\n",
    "    )\n",
    "    # learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5,\n",
    "    )\n",
    "\n",
    "    # training loop\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(PARAMETERS['num_epochs']):\n",
    "        # training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            X = batch['X'].to(DEVICE)  # (batch size, sequence_length, 442)\n",
    "            y = batch['y'].to(DEVICE)  # (batch size, 442)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X = batch['X'].to(DEVICE)\n",
    "                y = batch['y'].to(DEVICE)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # update learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        # handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name='Stock LSTM Hyperparameter Tuning', direction='minimize')\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TRIAL_TIMEOUT)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "with open(BEST_PARAMS_FILEPATH, 'w') as outfile:\n",
    "    json.dump(trial.params, outfile)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary>Stock LSTM Hyperparameter Tuning Study Trials</summary>\n",
    "  \n",
    "```  \n",
    "[I 2025-03-20 20:31:45,072] A new study created in memory with name: Stock LSTM Hyperparameter Tuning\n",
    "[I 2025-03-20 20:32:58,519] Trial 0 finished with value: 1.0372108221054077 and parameters: {'sequence_length': 147, 'hidden_dim': 234, 'num_layers': 1, 'dropout': 0.33930960583094977, 'learning_rate': 0.00021421665569423075, 'batch_size': 64, 'num_epochs': 60, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 1.0372108221054077.\n",
    "[I 2025-03-20 20:33:50,232] Trial 1 finished with value: 0.737079918384552 and parameters: {'sequence_length': 87, 'hidden_dim': 153, 'num_layers': 2, 'dropout': 0.4755953347734666, 'learning_rate': 0.0017851621886235617, 'batch_size': 16, 'num_epochs': 34, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:36:41,637] Trial 2 finished with value: 0.7578898668289185 and parameters: {'sequence_length': 39, 'hidden_dim': 136, 'num_layers': 3, 'dropout': 0.009576055372586267, 'learning_rate': 0.0001312467662147458, 'batch_size': 8, 'num_epochs': 77, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:37:34,095] Trial 3 finished with value: 1.0775812864303589 and parameters: {'sequence_length': 42, 'hidden_dim': 195, 'num_layers': 3, 'dropout': 0.4761042233260503, 'learning_rate': 0.00890964708789742, 'batch_size': 64, 'num_epochs': 80, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:37:49,886] Trial 4 finished with value: 1.087605595588684 and parameters: {'sequence_length': 23, 'hidden_dim': 124, 'num_layers': 1, 'dropout': 0.06866701681814286, 'learning_rate': 0.00011499798085947112, 'batch_size': 128, 'num_epochs': 61, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:37:51,422] Trial 5 pruned. \n",
    "[I 2025-03-20 20:37:53,880] Trial 6 pruned. \n",
    "[I 2025-03-20 20:39:41,367] Trial 7 finished with value: 0.9135304093360901 and parameters: {'sequence_length': 103, 'hidden_dim': 56, 'num_layers': 3, 'dropout': 0.46685690087632264, 'learning_rate': 0.005107658354735002, 'batch_size': 8, 'num_epochs': 65, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:39:43,281] Trial 8 pruned. \n",
    "[I 2025-03-20 20:39:44,910] Trial 9 pruned. \n",
    "[I 2025-03-20 20:39:49,927] Trial 10 pruned. \n",
    "[I 2025-03-20 20:43:06,551] Trial 11 finished with value: 0.7578939199447632 and parameters: {'sequence_length': 75, 'hidden_dim': 151, 'num_layers': 2, 'dropout': 0.20755764055341164, 'learning_rate': 0.0005332498443319924, 'batch_size': 8, 'num_epochs': 84, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:43:09,183] Trial 12 pruned. \n",
    "[I 2025-03-20 20:44:18,072] Trial 13 finished with value: 0.8719300031661987 and parameters: {'sequence_length': 50, 'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.0033603142132102156, 'learning_rate': 0.0006256986847047751, 'batch_size': 8, 'num_epochs': 50, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:44:24,776] Trial 14 pruned. \n",
    "[I 2025-03-20 20:44:26,131] Trial 15 pruned. \n",
    "[I 2025-03-20 20:46:49,218] Trial 16 finished with value: 0.8159942030906677 and parameters: {'sequence_length': 64, 'hidden_dim': 237, 'num_layers': 2, 'dropout': 0.117299654429115, 'learning_rate': 0.0012834787561936448, 'batch_size': 16, 'num_epochs': 91, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:46:51,984] Trial 17 pruned. \n",
    "[I 2025-03-20 20:46:54,608] Trial 18 pruned. \n",
    "[I 2025-03-20 20:46:56,151] Trial 19 pruned. \n",
    "[I 2025-03-20 20:46:58,613] Trial 20 pruned. \n",
    "[I 2025-03-20 20:47:06,870] Trial 21 pruned. \n",
    "[I 2025-03-20 20:47:10,176] Trial 22 pruned. \n",
    "[I 2025-03-20 20:50:28,116] Trial 23 finished with value: 0.8022396564483643 and parameters: {'sequence_length': 80, 'hidden_dim': 175, 'num_layers': 2, 'dropout': 0.007423100707065601, 'learning_rate': 0.0008267672586409669, 'batch_size': 8, 'num_epochs': 80, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:50:31,418] Trial 24 pruned. \n",
    "[I 2025-03-20 20:51:40,239] Trial 25 finished with value: 0.7693853974342346 and parameters: {'sequence_length': 36, 'hidden_dim': 154, 'num_layers': 1, 'dropout': 0.2070084265747843, 'learning_rate': 0.0014257234563781985, 'batch_size': 16, 'num_epochs': 87, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:51:44,262] Trial 26 pruned. \n",
    "[I 2025-03-20 20:51:50,934] Trial 27 pruned. \n",
    "[I 2025-03-20 20:51:53,548] Trial 28 pruned. \n",
    "[I 2025-03-20 20:51:55,345] Trial 29 pruned. \n",
    "[I 2025-03-20 20:51:58,871] Trial 30 pruned. \n",
    "[I 2025-03-20 20:53:06,713] Trial 31 finished with value: 0.8262477517127991 and parameters: {'sequence_length': 33, 'hidden_dim': 159, 'num_layers': 1, 'dropout': 0.21814272750012367, 'learning_rate': 0.001503948415731998, 'batch_size': 16, 'num_epochs': 88, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.737079918384552.\n",
    "[I 2025-03-20 20:53:58,362] Trial 32 finished with value: 0.7314378619194031 and parameters: {'sequence_length': 4, 'hidden_dim': 178, 'num_layers': 1, 'dropout': 0.23380603053231552, 'learning_rate': 0.0010570771786961055, 'batch_size': 16, 'num_epochs': 85, 'optimizer': 'RMSprop'}. Best is trial 32 with value: 0.7314378619194031.\n",
    "[I 2025-03-20 20:54:02,024] Trial 33 pruned. \n",
    "[I 2025-03-20 20:54:03,157] Trial 34 pruned. \n",
    "[I 2025-03-20 20:54:04,728] Trial 35 pruned. \n",
    "[I 2025-03-20 20:54:06,274] Trial 36 pruned. \n",
    "[I 2025-03-20 20:54:07,740] Trial 37 pruned. \n",
    "[I 2025-03-20 20:54:11,141] Trial 38 pruned. \n",
    "[I 2025-03-20 20:54:13,721] Trial 39 pruned. \n",
    "[I 2025-03-20 20:54:15,755] Trial 40 pruned. \n",
    "[I 2025-03-20 20:55:20,329] Trial 41 finished with value: 0.7441690564155579 and parameters: {'sequence_length': 29, 'hidden_dim': 161, 'num_layers': 1, 'dropout': 0.1877622038659314, 'learning_rate': 0.0010887960158424278, 'batch_size': 16, 'num_epochs': 87, 'optimizer': 'RMSprop'}. Best is trial 32 with value: 0.7314378619194031.\n",
    "[I 2025-03-20 20:55:23,229] Trial 42 pruned. \n",
    "[I 2025-03-20 20:56:24,333] Trial 43 finished with value: 0.7675689458847046 and parameters: {'sequence_length': 12, 'hidden_dim': 178, 'num_layers': 1, 'dropout': 0.1802991101020573, 'learning_rate': 0.000990289091537514, 'batch_size': 16, 'num_epochs': 95, 'optimizer': 'RMSprop'}. Best is trial 32 with value: 0.7314378619194031.\n",
    "[I 2025-03-20 20:56:28,421] Trial 44 pruned. \n",
    "[I 2025-03-20 20:56:29,740] Trial 45 pruned. \n",
    "[I 2025-03-20 20:56:33,723] Trial 46 pruned. \n",
    "[I 2025-03-20 20:56:35,405] Trial 47 pruned. \n",
    "[I 2025-03-20 20:56:40,337] Trial 48 pruned. \n",
    "[I 2025-03-20 20:56:42,941] Trial 49 pruned. \n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "```\n",
    "Study statistics: \n",
    " Number of finished trials:  50\n",
    "  Number of pruned trials:  35\n",
    "  Number of complete trials:  15\n",
    "Best trial:\n",
    "  Value:  0.7314378619194031\n",
    "  Params: \n",
    "    sequence_length: 4\n",
    "    hidden_dim: 178\n",
    "    num_layers: 1\n",
    "    dropout: 0.23380603053231552\n",
    "    learning_rate: 0.0010570771786961055\n",
    "    batch_size: 16\n",
    "    num_epochs: 85\n",
    "    optimizer: RMSprop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_history = plot_optimization_history(study)\n",
    "optimization_history.write_image('optimization_history.png')\n",
    "\n",
    "intermediate_values = plot_intermediate_values(study)\n",
    "intermediate_values.write_image('intermediate_values.png')\n",
    "\n",
    "parallel_coordinates = plot_parallel_coordinate(study)\n",
    "parallel_coordinates.write_image('parallel_coordinates.png')\n",
    "\n",
    "contour = plot_contour(study)\n",
    "contour.write_image('contour.png')\n",
    "\n",
    "slice_ = plot_slice(study)\n",
    "slice_.write_image('slice.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimisation History Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/optimization_history.png)\n",
    "\n",
    "The validation loss is shown as the objective value in the above optimisation history plot, with the lowest value being for study #32.\n",
    "Optimisation history shows the validation loss as the objective value, the lowest bein\n",
    "\n",
    "![Intermediate Values Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/intermediate_values.png)\n",
    "\n",
    "The intermediate values plot show the average validation loss per study as reported during the trial, which seems to plateau after around 40 epochs.\n",
    "\n",
    "![Parallel Coordinates Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/parallel_coordinates.png)\n",
    "\n",
    "The parallel coordinates plot is helpful to see correlations between the different hyperparameters and the objective function. The dark lines for lower objective value scores indicate the better trials, for example there is a strong correlation between a lower mean squared error loss and a batch size of 16.\n",
    "\n",
    "![Contour Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/contour.png)\n",
    "\n",
    "The contour plot visualises the relationship between hyperparameters by projecting it onto a 2D plane. For example, it suggests that a model with around 150 latent dimensions and a learning rate of 0.01 produces lower objective function values.\n",
    "\n",
    "![Slice Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/slice.png)\n",
    "\n",
    "The slice plot shows the objective function values for a single hyperparameter across all trials, for example RMSProp seems to perform better as an optimizer compared to SGD or Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Model with Best Hyperparameters\n",
    "\n",
    "This section uses the best parameters found via Optuna trials in the previous section to train a model from scratch over the entire dataset. The best parameters are:\n",
    "```json\n",
    "{\n",
    "    \"sequence_length\": 4,\n",
    "    \"hidden_dim\": 178,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.23380603053231552,\n",
    "    \"learning_rate\": 0.0010570771786961055,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 85,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_params = trial.params\n",
    "except:\n",
    "    with open('best_parameters.json', 'r') as infile:\n",
    "        best_params = json.load(infile)\n",
    "print('Using parameters:', best_params)\n",
    "\n",
    "dataset = StockDataset(\n",
    "    csv_file_path=TRAINING_DATA_FILEPATH,\n",
    "    in_seq_length=best_params['sequence_length'],\n",
    "    is_train=True,\n",
    "    train_val_split=0)\n",
    "data = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    shuffle=True)\n",
    "model = StockLSTM(\n",
    "    input_dim=N_COMPANIES,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = getattr(optim, best_params['optimizer'])(\n",
    "    model.parameters(),\n",
    "    lr=best_params['learning_rate'])\n",
    "train_losses = []\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data:\n",
    "        X = batch['X'].to(DEVICE)\n",
    "        y = batch['y'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['num_epochs']}, Loss: {avg_loss:.6f}\")\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "torch.save(model.state_dict(), BEST_MODEL_FILEPATH)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, best_params['num_epochs']+1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "  <summary>Training Loss Epochs</summary>\n",
    "\n",
    "  ```\n",
    "    Epoch 10/85, Loss: 0.667330\n",
    "    Epoch 20/85, Loss: 0.574235\n",
    "    Epoch 30/85, Loss: 0.536828\n",
    "    Epoch 40/85, Loss: 0.509404\n",
    "    Epoch 50/85, Loss: 0.480500\n",
    "    Epoch 60/85, Loss: 0.462396\n",
    "    Epoch 70/85, Loss: 0.449508\n",
    "    Epoch 80/85, Loss: 0.442076\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "![Training Loss Plot](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/training_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Make Predictions\n",
    "\n",
    "This section uses the trained model to make the predictions for 1st April using the previous `in_seq_length` days. The `StandardScaler` instances used for normalising is used to denormalise the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = dataset.in_seq_length\n",
    "last_sequence = dataset.normalized_data[:, -seq_length:]\n",
    "input_tensor = torch.FloatTensor(last_sequence.T).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction_norm = model(input_tensor).squeeze(0).cpu().numpy()\n",
    "predictions = prediction_norm\n",
    "for idx in range(len(predictions)):\n",
    "    predictions[idx] = dataset.scalers[idx].inverse_transform(predictions[idx].reshape(1, -1))\n",
    "submission = pd.DataFrame({\n",
    "    'ID': [f'company_{i}' for i in range(N_COMPANIES)],\n",
    "    'value': predictions.flatten()\n",
    "})\n",
    "submission.to_csv('submissions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the output directly for Kaggle submissions\n",
    "file = open('submissions.csv', 'r')\n",
    "data = file.readlines()\n",
    "file.close()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Interpretability\n",
    "\n",
    "This section uses the `Captum` library to understand the LSTM model similar to Lab 6. Since this is a time series prediction of multiple companies:\n",
    "* Attempt to understand which days from the lookback number of days are most attributed to predict the next day's output.\n",
    "* Attempt to understand how the output for a particular company attributes to other companies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_params = trial.params\n",
    "except:\n",
    "    with open('best_parameters.json', 'r') as infile:\n",
    "        best_params = json.load(infile)\n",
    "print('Using parameters:', best_params)\n",
    "model = StockLSTM(\n",
    "    input_dim=N_COMPANIES,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(torch.load(BEST_MODEL_FILEPATH, weights_only=False))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COMPANY = 18  # random company to generate attributions for\n",
    "TOP_K = 5\n",
    "\n",
    "model.eval()\n",
    "train_dataset = StockDataset(\n",
    "    csv_file_path=TRAINING_DATA_FILEPATH,\n",
    "    in_seq_length=best_params['sequence_length'],\n",
    "    is_train=True, \n",
    "    train_val_split=VAL_SPLIT\n",
    ")\n",
    "validation_dataset = StockDataset(\n",
    "    csv_file_path=TRAINING_DATA_FILEPATH,\n",
    "    in_seq_length=best_params['sequence_length'],\n",
    "    is_train=False, \n",
    "    train_val_split=VAL_SPLIT,\n",
    "    scalers=train_dataset.scalers\n",
    ")\n",
    "sample_data = validation_dataset[0]\n",
    "sample_X = sample_data['X']  # (seq_length, 442)\n",
    "sample_input = sample_X.unsqueeze(0).to(DEVICE)\n",
    "sample_input.requires_grad = True\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "attributions = ig.attribute(\n",
    "    inputs=sample_input,\n",
    "    baselines=torch.zeros_like(sample_input).to(DEVICE), \n",
    "    target=TARGET_COMPANY,\n",
    ")\n",
    "attributions = attributions.cpu().detach().numpy().squeeze(0)  # (seq_length, 442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise temporal attributions (which time steps matter most)\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_importance = np.abs(attributions).sum(axis=1)  # sum over companies\n",
    "plt.bar(range(best_params['sequence_length']), time_importance)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Attribution Magnitude')\n",
    "plt.title(f'Time Step Importance for Predicting Company {TARGET_COMPANY}')\n",
    "plt.xticks(range(best_params['sequence_length']))\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_step_attribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Time Step Attribution](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/time_step_attribution.png)\n",
    "\n",
    "Since the best parameters use four days as the lookback period to predict the output, the plot shows the importance of each of those 4 days. It suggests that the last (i.e. latest) day has the most influence on the next day's output, with decreasing influence over more previous days. The third-last and fourth-last days have almost similar influence. This model hence prioritises shorter time periods over longer term trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise which companies have the most influence\n",
    "plt.figure(figsize=(12, 8))\n",
    "company_importance = np.abs(attributions).mean(axis=0)\n",
    "top_indices = np.argsort(company_importance)[-TOP_K:]\n",
    "plt.barh(range(TOP_K), company_importance[top_indices])\n",
    "plt.yticks(range(TOP_K), top_indices)\n",
    "plt.xlabel('Attribution Magnitude')\n",
    "plt.ylabel('Company Index')\n",
    "plt.title(f'Top {TOP_K} Influential Companies for Predicting Company {TARGET_COMPANY}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('company_attribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Company Attribution](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/company_attribution.png)\n",
    "\n",
    "The plot aboves show how other nodes (companies) influence the output for a particular node (company). In this case, the most influential company to predict company 18's stock movement is company 18 itself. Other companies in the top 5 have similar attribution, but lesser than the target company itself. It could suggest that other companies perhaps belong to the same market sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise attribution patterns over time for top influential companies\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_indices = np.argsort(company_importance)[-TOP_K:]\n",
    "\n",
    "for i, idx in enumerate(top_indices):\n",
    "    plt.plot(range(best_params['sequence_length']),\n",
    "             attributions[:, idx], \n",
    "             label=f'Company {idx}',\n",
    "             marker='o',\n",
    "             linewidth=2)\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Attribution Value')\n",
    "plt.title(f'Attribution Patterns Over Time for Top {TOP_K} Influential Companies')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('company_influence.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Company Influence](https://raw.githubusercontent.com/siddydutta/LSTM-Stock-Market-Prediction/refs/heads/master/company_influence.png)\n",
    "\n",
    "The final plot shows each of the above five companies contribute numerically to the output for company 18. Interestingly, there is a divergence where two companies suggest a positive stock movement but three companies suggest a negative stock movement. The same company 18 has the strongest negative stock movement resulting in a final output of `-0.032038778`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Kaggle Submission (On Public Leaderboard)\n",
    "\n",
    "The model above achieved a score of `2.52854` on the public test dataset and was submitted under the team name: `Calls on $LSTM`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
